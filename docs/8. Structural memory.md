# Structural Memory Layer

## Purpose and Motivation

The **Structural Memory Layer** is responsible for **remembering semantic structures** that emerge inside the EventNetwork over time.

While the EventNetwork stores *facts* (events) and *semantic derivation relationships* (edges), the Structural Memory Layer captures **how meaning emerges repeatedly**, enabling:

- recognition of recurring derivation patterns
- detection of escalation and structural trends
- efficient pattern matching without repeated graph traversal
- future learning, adaptation, and rule evolution

This layer introduces **memory** into SYNAPSE — not memory of raw events, but memory of **semantic structure**.

---

## Conceptual Position in the Architecture

The Structural Memory Layer sits **above the EventNetwork** and **outside Rule logic**.

```
[ Ingest ]
    ↓
[ EventNetwork ]  ← immutable facts + derivation edges
    ↓
[ Rules ]         ← derive new events
    ↓
[ Structural Memory Layer ]  ← observe & remember structure

```

Key design principles:

- The EventNetwork does **not** know about memory
- Rules do **not** store memory
- Memory observes **completed derivations**
- Memory never mutates past events

This preserves clean separation of concerns and deterministic behavior.

---

## When Structural Memory Is Updated

Structural memory is updated **only at one precise semantic moment**:

> After a derived event is fully materialized, meaning:
>
> 1. the derived event node is added
> 2. all contribution edges are added
> 3. the rule evaluation completes successfully

This moment is referred to as:

> Derived Event Materialization
>

Memory is **never updated**:

- during partial evaluation
- while edges are still missing
- for failed or unsatisfied rules

This guarantees that stored structures are **stable, complete, and meaningful**.

---

## Structural Observation Hook

The Structural Memory Layer is triggered via a single observer-style hook:

```go
OnDerivedEventMaterialized(
    derived Event,
    contributors []Event,
    ruleID string,
)

```

This hook:

- receives the derived event
- receives all contributing events
- knows which rule produced the derivation

It does **not**:

- modify the network
- trigger new rules
- evaluate conditions

Its role is purely observational and mnemonic.

---

## Structural Memory Primitives

### Structural Revision Tracking

To support caching and invalidation, the layer tracks **structural revisions**, such as:

- global network revision
- per-node inbound/outbound revision
- derivation-level depth changes

These revisions allow the system to know **when cached structural knowledge becomes stale**.

---

### Local Semantic Statistics

For each derived event, the system records lightweight semantic summaries, for example:

- count of contributor event types
- contributor domains
- depth of derivation
- temporal density

These summaries allow many expression checks (e.g. `HasChild`, `HasDescendants`) to be resolved **without traversing the graph**.

---

### Motif (Structural Pattern) Memory

The core unit of structural memory is a **Motif**.

A motif represents a **semantic derivation shape**, independent of specific event IDs.

A motif key typically includes:

- derived event type
- domain
- normalized contributor types and relations

Example (conceptual):

```
Derived: cpu_critical
Contributors: [cpu_status_changed, cpu_status_changed, cpu_status_changed]
Relation: trigger

```

Multiple derivations can map to the **same motif**, forming a history of pattern occurrences.

---

### Motif Instances

Each time a motif occurs, the layer records a **motif instance**, including:

- derived event ID
- contributor event IDs
- timestamp
- originating rule

This enables:

- frequency analysis
- temporal clustering
- escalation detection
- dominance analysis of contributors

---

## What Structural Memory Enables

With structural memory in place, SYNAPSE can:

- detect repeated semantic situations
- recognize escalation patterns
- identify dominant contributors to derived states
- short-circuit expensive graph traversals
- enable future learning and adaptive rule behavior

Importantly, this is done **without modifying past events** and **without recomputing structure repeatedly**.

#### Pattern Recognition and Stabilization

Structural memory enables recognition of repeated semantic derivations over time.
When recurrence thresholds are exceeded, these recognitions may be promoted into new derived events,
representing stabilized semantic state rather than momentary interpretation.

> Rules construct meaning.
> Patterns stabilize meaning.

> Recognition answers whether meaning is structurally valid.
Stabilization answers whether meaning is durable.

---

## What Structural Memory Is Not

The Structural Memory Layer is explicitly **not**:

- a rule engine
- a causal inference system
- a CEP window processor
- a mutable knowledge base

It does not explain *why* something happened.

It remembers *how meaning emerged*.

---

## Alignment with SYNAPSE Philosophy

This layer embodies the core SYNAPSE idea:

> Meaning is not stored in events themselves,
>
>
> but in the **structure of their relationships over time**.
>

Structural memory allows SYNAPSE to move from:

- event processing → semantic accumulation
- rule execution → pattern recognition
- stateless derivation → remembered structure

This forms the foundation for future extensions such as:

- adaptive rules
- structural learning
- emergent behavior analysis

---

## Scope and Status

The current Structural Memory Layer is implemented as a **conceptual POC**:

- in-memory
- deterministic
- non-optimized

It is intentionally designed to evolve independently of storage, scale, or performance strategies.
